# Test Runner CLI — Running Tests via `--test` Flag

## Overview

The `--test` flag enables running test suites directly from the CLI, printing results to console. This provides a way to validate code through automated testing without manually invoking the REPL's `import { run } from 'node:test'` API.

**Key Insight**: Tests execute in the browser/worker environment against live application state, combining the power of the REPL with structured test validation.

---

## Progress and Current State

### ⚠️ Incomplete Implementation: Streaming Not Functional

The test runner infrastructure is **substantially complete** but **missing intermediate progress streaming during test execution**. This is a critical gap from the specification defined in `docs/1.3-workers-and-test-runner.md`.

#### What Works ✓
- Test file discovery via `/daebug/discover-tests` endpoint (server.js:319-343)
- Test execution in browser/worker contexts (js/modules/test-runner.js:108-251)
- Test result collection and formatting (test.template.js:71-105)
- Final results delivery after test run completes
- Server-side handlers ready to receive and process streaming updates (server.js:353-378, writer.js:57-75)

#### What's Missing ✗
- **Intermediate progress updates during test execution** (spec requirement: docs/1.3-workers-and-test-runner.md:301-335)
- Progress streaming is implemented in structure but **never invoked during the test loop**
- `streamProgress()` function defined in `js/modules/test-runner.js:313-328` but called only once at completion (line 331)
- No mechanism for `daebugRunTests()` (lines 108-251) to report progress back to the streaming function
- Result of: **Scope isolation** — `streamProgress()` nested inside `run()`, unreachable from test execution logic

#### Specification vs. Reality

| Requirement | Specification | Current State |
|-------------|---------------|---------------|
| **Streaming Frequency** | Debounced ~1-5 seconds during test execution | Only single update at completion |
| **Progress Data** | `recentTests[]` array + running totals | Only `allTests[]` sent at end |
| **REPL User Experience** | Chat log updated incrementally as tests run | Chat log unchanged until all tests finish |
| **Long Test Awareness** | User sees feedback during long test runs | No feedback; appears frozen if >5 seconds |

#### Root Cause

The test execution (`daebugRunTests()`) and streaming orchestration (`run()`) exist in separate scopes with no communication pathway:

```javascript
// js/modules/test-runner.js

export async function daebugRunTests(options = {}) {
  // Lines 108-251: Executes tests, accumulates results
  // NO streaming happens here
  // Can't access streamProgress() - it doesn't exist yet
}

export async function run(options = {}) {
  // Lines 253-340: Orchestrates discovery and execution
  
  const streamProgress = async (progressData) => {
    // Lines 313-328: Defined here, locally scoped
    // Never called during test loop
    await fetch('/daebug/test-progress', {...});
  };
  
  const results = await daebugRunTests({...});
  
  // Only called ONCE after completion
  await streamProgress({
    complete: true,
    allTests: results.tests
  });
}
```

### Next Steps to Complete Streaming Implementation

1. **Refactor to expose streaming callback**:
   - Move `streamProgress()` outside of `run()` or pass it as a callback parameter
   - Make it accessible to `daebugRunTests()` as an optional streaming handler
   - Signature: `streamProgress(progressData)` where progressData includes current totals and test batch

2. **Integrate intermediate streaming into test execution loop**:
   - In `daebugRunTests()` after each test completes (line 228), check if streaming callback provided
   - If available: accumulate recent test(s) and debounce stream calls (~1-5 seconds)
   - Track `lastProgressTime` to implement debouncing
   - Include `complete: false` in intermediate updates, only set `complete: true` on final call

3. **Update progress data structure**:
   - Intermediate updates: include `recentTests[]` (recently-completed tests only) + `totals` + `duration`
   - Final update: include `allTests[]` (all tests) + `totals` + `duration` + `complete: true`

4. **Test the implementation**:
   - Run test suite with 10+ tests and verify progress updates appear in REPL log at ~2-5 second intervals
   - Verify final results are complete and accurate
   - Verify formatting matches spec (pass/fail/skip counts, test names, error stacks)

---

## Syntax

```
daebug --test [<test-file>] [--page=<page-name> | --page <page-name>]
```

### Parameters

- **`<test-file>`** *(optional)* — Filter to run only matching test file(s)
  - Glob patterns supported (e.g., `**/*.test.js`, `client/**/*.test.js`)
  - If omitted, discovers and runs all test files in the project
  - Relative to project root

- **`--page=<page-name>` or `--page <page-name>`** *(optional)* — Target specific page or worker realm
  - Page name is matched against registered realms in the active daebug session
  - Partial matching: picks first realm that contains specified substring
  - Preference: pages preferred over web workers
  - If omitted: auto-selects first available page (preferring non-workers)

### Return Values

- Prints test results to console in structured format (pass/fail counts, timing, individual test outcomes)
- Exit code: `0` if all tests pass, `1` if any test fails or errors occur
- Results also streamed to the REPL log for the selected page

---

## Behavior

### Execution Flow

1. **Page Selection**
   - Queries daebug server for active pages/realms
   - If `--page` specified: matches against realm names (substring search, prefer non-workers)
   - If `--page` omitted: selects first available realm (preference order: pages > workers)
   - Fails if no realms available

2. **Test File Discovery**
   - If `<test-file>` specified: runs only that file
   - If omitted: discovers all `*.test.js` files in project
   - Respects project root boundaries (does not escape via `..`)

3. **Test Execution**
   - Injects test runner into selected page/worker via REPL command
   - Tests execute in browser context (DOM access, timers, async APIs available)
   - Results collected and formatted as structured markdown

4. **Output**
   - Console displays summary: `✓ X pass, ✗ Y fail, ⊙ Z skip` plus timing
   - Individual test results shown with pass/fail status and error stacks
   - REPL chat log updated with full results

---

## Examples

### Run All Tests (First Available Page)

```bash
daebug --test
```

Output:
```
👾Daebug v0.0.1 listening on http://localhost:8342/
Discovering test files...
Found 16 test files

Running tests on page: 7-zen-1201-03

## Test Results: 36 pass, 14 fail, 0 skip (82ms)

✓ parseRequest extracts code from footer-based request
✓ parseRequest extracts agent from header
... (34 more passes)
✗ Import: /js/client-helpers.test.js
  SyntaxError: The requested module 'node:assert' does not provide an export named 'strict'
... (13 more failures)
```

### Run Single Test File

```bash
daebug --test js/parser.test.js
```

### Run Tests with Glob Pattern

```bash
daebug --test "**/*.test.js" --page=main
```

Runs all test files matching the glob, targeting the page named "main" (or first page containing "main").

### Run Tests on Specific Worker

```bash
daebug --test --page=webworker
```

Targets any realm with "webworker" in its name. If multiple match, selects the first one encountered.

---

## Integration with REPL

### REPL Command Equivalent

Running `daebug --test` is equivalent to executing in the REPL:

```javascript
const { run } = await import('node:test');
await run({ files: ['js/**/*.test.js'] });
```

However, the CLI version:
- Handles page/worker selection automatically
- Parses and formats results for console output
- Sets appropriate exit code
- Works without interactive REPL session

### Accessing Results Programmatically

Test results are written to the REPL chat log (`daebug/<page-name>.md`) in structured markdown format:

```markdown
## Test Results: 36 pass, 14 fail, 0 skip (82ms)

✓ Test Name (5ms)
✗ Failed Test
  Error: Details here
  Stack trace...
```

---

## Page Selection Algorithm

### Priority Order

1. **Explicit `--page` match**
   - Substring search (case-insensitive): first realm where name contains specified text
   - Prefer pages over workers: if "foo" matches both "foo-page" and "foo-worker", pick "foo-page"

2. **Auto-selection (no `--page`)**
   - Query active daebug session for all registered realms
   - Filter to pages only (exclude workers)
   - If no pages: use any available worker
   - Select by most-recently-active (highest `lastSeen` timestamp)

### Examples

**Active Realms**: `7-zen-1201-03`, `7-zen-1201-03-webworker`, `8-dune-2040-42`

```bash
daebug --test                    # → selects 8-dune-2040-42 (most recent page)
daebug --test --page=zen         # → selects 7-zen-1201-03 (first match, not worker)
daebug --test --page=webworker   # → selects 7-zen-1201-03-webworker (explicit match)
daebug --test --page=dune        # → selects 8-dune-2040-42 (substring match)
```

---

## Error Handling

### No Active Realms

```
Error: No active pages or workers found.
Ensure daebug server is running with `npx daebug`.
```

### Page Not Found

```
Error: No page matching "xyz" found.
Active pages: 7-zen-1201-03, 8-dune-2040-42
```

### Test File Not Found

```
Error: No test files matching "xyz/*.test.js" found.
Search path: /path/to/project/
```

### Server Connection Failed

```
Error: Could not connect to daebug server on port 8342.
Ensure server is running: npx daebug
```

---

## Implementation Notes

### CLI Integration

The `--test` flag is handled in `js/cli.js` `parseArgs()` function:

```javascript
{
  test: false,        // boolean flag
  testFile: null,     // optional test file/glob
  page: null,         // optional page name filter
  help: false,
  version: false,
  root: null,
  port: null
}
```

### Test Discovery

Test discovery queries the server API:
- Endpoint: `POST /daebug/discover-tests` (existing or new)
- Request: `{ root, pattern }`
- Response: `{ files: [...], error?: string }`

### Results Formatting

Results formatted in markdown following Node.js test runner conventions:
- Summary line: `## Test Results: X pass, Y fail, Z skip (Zms)`
- Per-test lines with indentation and status icons

---

## Relationship to REPL Testing

| Aspect | REPL | CLI `--test` |
|--------|------|-------------|
| **Invocation** | Interactive commands in chat log | Single CLI command |
| **File Discovery** | Manual file paths | Automatic glob discovery |
| **Page Selection** | Manual realm name | Auto-selected or filtered |
| **Results Display** | Streamed to chat log | Console output + chat log |
| **Use Case** | Exploration, debugging | CI/CD, validation, scripting |

Both use the same underlying test runner (`js/modules/test-runner.js`), ensuring consistency.
